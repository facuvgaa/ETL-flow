# Airflow Song & Weather Flow ğŸ¶ğŸŒ¦ï¸

## ğŸ‡ªğŸ‡¸ DescripciÃ³n (EspaÃ±ol)

Este proyecto implementa un **pipeline de datos completo**, desde la ingesta hasta el entrenamiento de modelos de Machine Learning, todo orquestado con **Apache Airflow** y desplegado en **AWS con Terraform**.  

AdemÃ¡s, un entorno de **Jupyter Notebook dockerizado** permite consumir datos desde **Amazon Redshift**, entrenar modelos y registrarlos en **MLflow**.

---

## ğŸš€ Componentes principales

1. **Infraestructura (Terraform)**
   - Crea en AWS:
     - Buckets de **S3** para data lake.
     - **Glue jobs** para uniÃ³n y migraciÃ³n a Redshift.
     - **Redshift Cluster** para data warehouse.
     - Roles y polÃ­ticas de **IAM**.
     - **Secrets Manager** para credenciales seguras.

2. **OrquestaciÃ³n de ETLs (Airflow)**
   - Pipelines que:
     - **Extraen** datos de clima usando la API de [OpenWeatherMap](https://openweathermap.org/) (**requiere API key**) a travÃ©s de [`facu-weather-flow`](https://pypi.org/project/facu-weather-flow/).  
     - **Extraen** datos de Spotify (**requiere Client ID, Client Secret y ID de la playlist a extraer**) a travÃ©s de [`track-flow`](https://pypi.org/project/track-flow/).  
     - **Transforman** los datos a formato **Parquet**.
     - **Cargan** en **S3** y **Redshift**.

3. **Entrenamiento de Modelos (Jupyter + MLflow)**
   - Jupyter Notebook se conecta a **Redshift**.
   - Se entrenan modelos de Machine Learning.
   - Los modelos se registran en **MLflow Tracking Server** (dockerizado).

---

## ğŸ› ï¸ TecnologÃ­as

- Apache Airflow, Terraform, AWS (S3, Redshift, Glue, Secrets Manager)  
- MLflow, Jupyter Notebook  
- Docker & docker-compose  
- Spark jobs y Python jobs  
- LibrerÃ­as personalizadas: [`facu-weather-flow`](https://pypi.org/project/facu-weather-flow/), [`track-flow`](https://pypi.org/project/track-flow/)

## ğŸ“‚ Estructura

```bash
â”œâ”€â”€ ansible/              # Playbooks y roles de Ansible
â”œâ”€â”€ dags/                 # DAGs y configuraciÃ³n de Airflow
â”œâ”€â”€ infra/                # Infraestructura como cÃ³digo (Terraform)
â”œâ”€â”€ mlflow/               # ConfiguraciÃ³n de MLflow para experiment tracking
â”œâ”€â”€ docker-compose.yml    # OrquestaciÃ³n local con Docker Compose
â”œâ”€â”€ flow.excalidraw       # Diagrama del flujo del proyecto
â”œâ”€â”€ generate_env.py       # Script para generar archivos .env automÃ¡ticamente
â”œâ”€â”€ install.sh            # Instalador de dependencias bÃ¡sicas (Docker, Compose, Make)
â”œâ”€â”€ requirements.txt      # Dependencias de Python
â””â”€â”€ README.md             # DocumentaciÃ³n principal
```

# 1. Instalar dependencias bÃ¡sicas (Docker, Docker Compose y Make)
```bash
./install.sh
```
# 2. Configurar variables de entorno

 ### Copiar el archivo de ejemplo a .env
```bash
cp infra/.example.env infra/.env
```

 ## Editar infra/.example.env y poner al menos tus credenciales de AWS, las credenciles de  Openweathermap , credenciales spotify, junto con el chart para extraer info de spotify: 
```bash
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_DEFAULT_REGION=us-east-1

OPENWEATHERMAP_API_KEY=
CITY_DATA=Tucuman,AR


#spotify api

CLIENT_ID_SPOTIFY=
CLIENT_SECRET_SPOTIFY=
TRACK_LIST=2U3UUpx6ocHXgvcXmq0YBw

```


# 3. Levantar la infraestructura con Terraform (vÃ­a Docker + Make)
```bash
# Inicializar Terraform
make infra

# Revisar el plan de ejecuciÃ³n
make plan

# Aplicar cambios (crear la infraestructura en AWS)
make apply
```

# 4. Levantar Airflow y MLflow (Docker local)

```bash
# Levantar Airflow
docker-compose up -d airflow

# Acceder a Airflow: http://localhost:8080

# Levantar MLflow
docker-compose up -d mlflow

# Acceder a MLflow: http://localhost:5000
```

# 5. Ejecutar DAGs y entrenar modelos

1. Airflow detectarÃ¡ los DAGs dentro de dags/.

2. Ejecutar los DAGs desde UI o CLI.

3. Los datos se cargarÃ¡n en S3 y Redshift.

4. Abrir Jupyter Notebook y entrenar modelos conectÃ¡ndose a Redshift.

5. Registrar modelos en MLflow.


6. Notas finales

Proyecto pensado para pruebas locales y despliegue en AWS.

Recomendado Python 3.12.

Mantener librerÃ­as actualizadas:

